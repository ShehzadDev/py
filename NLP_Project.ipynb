{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShehzadDev/py/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping and Data Manipulation by NLTK\n",
        "\n",
        "In this Colab notebook, we'll explore how to perform web scraping using Python with the help of the `requests` library for fetching web content and `BeautifulSoup` for parsing HTML. Additionally, we'll use the `pandas` library to manipulate and organize the extracted data in dataframes\n",
        "\n"
      ],
      "metadata": {
        "id": "3bd51MxH1XXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PMlHSxKUyYvs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coursera"
      ],
      "metadata": {
        "id": "UTYCgpECMEVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bJZHh8H_YmEP"
      },
      "outputs": [],
      "source": [
        "# def scrape_coursera(topic):\n",
        "#     url = 'https://www.coursera.org/courses?query={topic}'\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "#     # Extract relevant information from the Coursera page\n",
        "#     course_titles = soup.find_all('div', class_='course-title')\n",
        "#     course_descriptions = soup.find_all('div', class_='course-description')\n",
        "#     course_levels = soup.find_all('div', class_='course-level')\n",
        "#     course_prices = soup.find_all('div', class_='course-price')\n",
        "\n",
        "#     # Create a DataFrame with the extracted data\n",
        "#     coursera_data = pd.DataFrame({\n",
        "#         'Course Title': course_titles,\n",
        "#         'Course Description': course_descriptions\n",
        "#     })\n",
        "\n",
        "#     # Convert the coursera_data variable to a string value\n",
        "#     coursera_data['Course Title'] = coursera_data['Course Title'].to_string()\n",
        "#     coursera_data['Course Description'] = coursera_data['Course Description'].to_string()\n",
        "\n",
        "#     return coursera_data\n",
        "\n",
        "# # Call the function to scrape Coursera data\n",
        "# coursera_data = scrape_coursera('computer science')\n",
        "# print(coursera_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Udemy"
      ],
      "metadata": {
        "id": "tIoE8xwaMJNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tai4l8f5ZBbB"
      },
      "outputs": [],
      "source": [
        "# def scrape_udemy():\n",
        "#     url = 'https://www.udemy.com/courses/search/?q=computer%20science'\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "#     # Extract relevant information from the Udemy page\n",
        "#     course_titles = soup.find_all('h2', class_='course-title')\n",
        "#     course_descriptions = soup.find_all('div', class_='course-description')\n",
        "#     course_prices = soup.find_all('div', class_='course-price')\n",
        "\n",
        "#     # Create a DataFrame with the extracted data\n",
        "#     udemy_data = pd.DataFrame({\n",
        "#         'Course Title': course_titles,\n",
        "#         'Course Description': course_descriptions\n",
        "#     })\n",
        "\n",
        "#     return udemy_data\n",
        "\n",
        "# # Call the function to scrape Udemy data\n",
        "# udemy_data = scrape_udemy()\n",
        "\n",
        "# print(udemy_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCW"
      ],
      "metadata": {
        "id": "gjLrmddiMNMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bK9uU0doZVLN"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import fitz  # PyMuPDF\n",
        "# import pandas as pd\n",
        "\n",
        "# def extract_title_from_text(text):\n",
        "#     lines = text.split('\\n')\n",
        "#     title = lines[0].strip() if lines else 'Unknown Title'\n",
        "#     return title\n",
        "\n",
        "# def scrape_ocw(file_paths):\n",
        "#     # Create lists to store data\n",
        "#     topic_titles = []\n",
        "#     topic_contents = []\n",
        "\n",
        "#     for file_path in file_paths:\n",
        "#         # Extract relevant information from the PDF file\n",
        "#         with fitz.open(file_path) as pdf_document:\n",
        "#             # Assuming each page in the PDF is a separate topic\n",
        "#             for page_num in range(pdf_document.page_count):\n",
        "#                 page = pdf_document[page_num]\n",
        "#                 text = page.get_text()\n",
        "#                 title = extract_title_from_text(text)\n",
        "#                 topic_titles.append(title)\n",
        "#                 topic_contents.append(text)\n",
        "\n",
        "#     # Create a DataFrame with the extracted data\n",
        "#     ocw_data = pd.DataFrame({\n",
        "#         'Topic Title': topic_titles,\n",
        "#         'Topic Content': topic_contents\n",
        "#     })\n",
        "\n",
        "#     return ocw_data\n",
        "\n",
        "# # Example usage with a list of locally downloaded PDF file paths\n",
        "# pdf_file_paths = [\n",
        "#     '/content/2c91bd942816c0cca14f216f098c0bf4_MIT6_857S14_Lec01.pdf',\n",
        "#     '/content/49ff5c8ba85258a7e9da7e6463687420_MIT6_857S14_Lec03.pdf',\n",
        "#     '/content/af613d5bbf100539a9ac8c05ecfd1b64_MIT6_857S14_Lec05.pdf',\n",
        "#     '/content/7fe96705a82149e2de1d5f20ca34595b_MIT6_857S14_Lec02.pdf',\n",
        "#     '/content/a526654fdcc273259d1e2bc5b4989d4d_MIT6_857S14_Lec04.pdf',\n",
        "#     '/content/b5053bc3f3c29468545043e2666b0764_MIT6_857S14_Lec10.pdf',\n",
        "#     '/content/b5053bc3f3c29468545043e2666b0764_MIT6_857S14_Lec10.pdf'\n",
        "# ]\n",
        "\n",
        "# ocw_data = scrape_ocw(pdf_file_paths)\n",
        "\n",
        "# # Display the DataFrame with 'Topic Title' and 'Topic Content'\n",
        "# print(ocw_data)\n",
        "\n",
        "# # Save the DataFrame to a CSV file\n",
        "# ocw_data.to_csv(\"ocw.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Topic Scraping\n",
        "\n",
        "\n",
        "1. **Fetching Page Titles:**\n",
        "   - The script constructs Wikipedia API URLs to search for each topic's page titles.\n",
        "   - Requests are made to the API, and the response data is converted to JSON format.\n",
        "   - Relevant information (page titles) is extracted from the search results.\n",
        "\n",
        "2. **Fetching Page Content:**\n",
        "   - Using the obtained page titles, new Wikipedia API URLs are constructed to fetch page content.\n",
        "   - Requests are made to the API, and the response data is converted to JSON format.\n",
        "   - Relevant information (page content) is extracted from the content results.\n",
        "\n",
        "3. **Creating DataFrame:**\n",
        "   - The extracted page titles, along with the corresponding content, are organized into a pandas DataFrame.\n",
        "   - The DataFrame includes columns for 'Topic ID', 'Topic Name', and 'Topic Content Description'.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NP0a2XuS2acI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmfQTe8vZfFL",
        "outputId": "eb19e683-a3f9-45fe-b922-16583ab0ffcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Topic Title  \\\n",
            "0               Computer network   \n",
            "1     Port (computer networking)   \n",
            "2               Wireless network   \n",
            "3    Computer Networks (journal)   \n",
            "4               Computer science   \n",
            "..                           ...   \n",
            "85                    Rel (DBMS)   \n",
            "86  Isolation (database systems)   \n",
            "87               Object database   \n",
            "88               Oracle Database   \n",
            "89    Open Database Connectivity   \n",
            "\n",
            "                                        Topic Content  \\\n",
            "0   A computer network is a set of computers shari...   \n",
            "1   In computer networking, a port or port number ...   \n",
            "2   A wireless network is a computer network that ...   \n",
            "3   Computer Networks is a scientific journal of c...   \n",
            "4   Computer science is the study of computation, ...   \n",
            "..                                                ...   \n",
            "85  Rel is an open-source true relational database...   \n",
            "86  In database systems, isolation determines how ...   \n",
            "87  An object database or object-oriented database...   \n",
            "88  Oracle Database (commonly referred to as Oracl...   \n",
            "89  In computing, Open Database Connectivity (ODBC...   \n",
            "\n",
            "                                                 Urls  \n",
            "0   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "1   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "2   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "3   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "4   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "..                                                ...  \n",
            "85  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "86  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "87  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "88  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "89  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "\n",
            "[90 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "def scrape_wikipedia(topics):\n",
        "\n",
        "    # Create a list of Wikipedia API URLs for each topic to get page titles\n",
        "    titles_urls = [f'https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={topic}' for topic in topics]\n",
        "\n",
        "    # Make requests to the Wikipedia API to get page titles\n",
        "    titles_responses = [requests.get(url) for url in titles_urls]\n",
        "\n",
        "    # Convert the response data to JSON format\n",
        "    titles_data = [response.json() for response in titles_responses]\n",
        "\n",
        "    # Extract relevant information from the Wikipedia search results\n",
        "    page_titles = [result['title'] for d in titles_data for result in d['query']['search']]\n",
        "\n",
        "    # Create a list of Wikipedia API URLs for each topic to get page content\n",
        "    content_urls = [f'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&titles={title}&exintro&explaintext' for title in page_titles]\n",
        "\n",
        "    # Make requests to the Wikipedia API to get page content\n",
        "    content_responses = [requests.get(url) for url in content_urls]\n",
        "    # print(len(content_urls))\n",
        "\n",
        "    # Convert the response data to JSON format\n",
        "    content_data = [response.json() for response in content_responses]\n",
        "\n",
        "    # Extract relevant information from the Wikipedia content results\n",
        "    page_contents = [next(iter(d['query']['pages'].values()))['extract'] for d in content_data]\n",
        "    # Create a DataFrame with the extracted data\n",
        "    wikipedia_data = pd.DataFrame({'Topic Title': page_titles, 'Topic Content': page_contents,'Urls':content_urls})\n",
        "\n",
        "    return wikipedia_data\n",
        "\n",
        "# Example usage\n",
        "topics = ['Computer Networks', 'Web Development', 'Machine Learning', 'Natural Language processing',\n",
        "          'OOP', 'Programming Fundamentals', 'DSA', 'Object Oriented Analysis and Design', 'DBMS']\n",
        "\n",
        "wikipedia_data = scrape_wikipedia(topics)\n",
        "\n",
        "# Display the DataFrame with titles and content\n",
        "print(wikipedia_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "098dIEdcquNs",
        "outputId": "67c8c7c2-1999-4479-cc02-c025c0cc50ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Topic Title', 'Topic Content', 'Urls'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SqX0P2aZqPx",
        "outputId": "474d1bf4-f832-41ef-9056-e74812a79e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Topic Title  \\\n",
            "0               Computer network   \n",
            "1     Port (computer networking)   \n",
            "2               Wireless network   \n",
            "3    Computer Networks (journal)   \n",
            "4               Computer science   \n",
            "..                           ...   \n",
            "85                    Rel (DBMS)   \n",
            "86  Isolation (database systems)   \n",
            "87               Object database   \n",
            "88               Oracle Database   \n",
            "89    Open Database Connectivity   \n",
            "\n",
            "                                        Topic Content  \\\n",
            "0   A computer network is a set of computers shari...   \n",
            "1   In computer networking, a port or port number ...   \n",
            "2   A wireless network is a computer network that ...   \n",
            "3   Computer Networks is a scientific journal of c...   \n",
            "4   Computer science is the study of computation, ...   \n",
            "..                                                ...   \n",
            "85  Rel is an open-source true relational database...   \n",
            "86  In database systems, isolation determines how ...   \n",
            "87  An object database or object-oriented database...   \n",
            "88  Oracle Database (commonly referred to as Oracl...   \n",
            "89  In computing, Open Database Connectivity (ODBC...   \n",
            "\n",
            "                                                 Urls  \n",
            "0   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "1   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "2   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "3   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "4   https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "..                                                ...  \n",
            "85  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "86  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "87  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "88  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "89  https://en.wikipedia.org/w/api.php?action=quer...  \n",
            "\n",
            "[90 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Concatenate all the dataframes into a single dataframe\n",
        "# corpus = pd.concat([ ocw_data , wikipedia_data], ignore_index=True)\n",
        "corpus=copy.deepcopy(wikipedia_data)\n",
        "\n",
        "# Display the combined dataframe\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lhn5X_Saoxj"
      },
      "source": [
        "# Text Data Cleaning\n",
        "\n",
        "1. **Remove Extra Whitespaces:**\n",
        "   - Utilizes the `split` and `join` functions to eliminate additional whitespaces, ensuring text is well-formatted.\n",
        "\n",
        "2. **Remove Special Symbols and Non-Alphanumeric Characters:**\n",
        "   - Applies a regular expression (`re.sub`) to keep only alphanumeric characters and spaces, discarding special symbols.\n",
        "\n",
        "3. **Remove and Replace Broken Lines:**\n",
        "   - Uses a regular expression to remove newline characters (`\\n`) and replaces them with a space to maintain continuity.\n",
        "\n",
        "4. **Convert to Lowercase:**\n",
        "   - Transforms the entire text to lowercase to achieve uniformity in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTTGP_tmaqAy",
        "outputId": "445efef8-9ea1-4f06-b85b-22b69baf88d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     a computer network is a set of computers shari...\n",
            "1     in computer networking a port or port number i...\n",
            "2     a wireless network is a computer network that ...\n",
            "3     computer networks is a scientific journal of c...\n",
            "4     computer science is the study of computation i...\n",
            "                            ...                        \n",
            "85    rel is an opensource true relational database ...\n",
            "86    in database systems isolation determines how t...\n",
            "87    an object database or objectoriented database ...\n",
            "88    oracle database commonly referred to as oracle...\n",
            "89    in computing open database connectivity odbc i...\n",
            "Name: Topic Content, Length: 90, dtype: object\n",
            "Index(['Topic Title', 'Topic Content', 'Urls'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "def clean_text(text):\n",
        "    # Remove extra whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Remove special symbols and non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Remove and replace broken lines\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Assuming 'wikipedia_data' is your DataFrame and it contains a 'Topic Content' column\n",
        "corpus['Topic Content'] = corpus['Topic Content'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(corpus['Topic Content'])\n",
        "print(corpus.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=[]\n",
        "word_count=[]\n",
        "for content in corpus['Topic Content']:\n",
        "  vocab_size.append(len(set(content.split(' '))))\n",
        "  word_count.append(len(content.split(' ')))\n",
        "corpus['vocabulary size'],corpus['word count']=vocab_size,word_count\n",
        "corpus.to_csv('/content/corpus.csv')"
      ],
      "metadata": {
        "id": "fgWUo-6cvJu5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sZy0lWIPF_f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.columns"
      ],
      "metadata": {
        "id": "zjoyqbs8xQ08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqAqrcQabkb7"
      },
      "source": [
        "# Text Tokenization with NLTK in Python\n",
        "\n",
        "## Importing NLTK and Downloading Resources\n",
        "\n",
        "The code begins by importing the necessary module for tokenization from NLTK (`word_tokenize`). Additionally, the `nltk.download('punkt')` command ensures that the required 'punkt' resource is downloaded. This resource contains pre-trained models for tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=copy.deepcopy(corpus[['Topic Title', 'Topic Content']])"
      ],
      "metadata": {
        "id": "KWqD_w9xxBhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokens.csv: topic, stems, lemmas"
      ],
      "metadata": {
        "id": "zBA0EaZbxUxE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBoh6mq2biL5"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to tokenize text data\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Assuming 'wikipedia_data' is your DataFrame and it contains a 'Cleaned Content' column\n",
        "tokens['Tokens'] = tokens['Topic Content'].apply(tokenize_text)\n",
        "\n",
        "# Display the tokenized DataFrame\n",
        "print(tokens[['Topic Content', 'Tokens']])\n",
        "print(tokens.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Qpk-3db-zT"
      },
      "source": [
        "# Text Preprocessing with NLTK: Stemming and Lemmatization\n",
        "\n",
        "## Importing NLTK and Downloading Resources\n",
        "\n",
        "The code begins by importing the required modules from NLTK (`PorterStemmer`, `WordNetLemmatizer`, and `stopwords`). It also ensures that the necessary NLTK resources ('stopwords' and 'wordnet') are downloaded.\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXk9gKdMcFBr"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to perform stemming\n",
        "def stem_text(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def lemmatize_text(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Remove stopwords and apply stemming and lemmatization to the 'Tokens' column\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens['Stemmed Tokens'] = tokens['Tokens'].apply(lambda x: [token for token in x if token not in stop_words])\n",
        "tokens['Stemmed Tokens'] = tokens['Stemmed Tokens'].apply(stem_text)\n",
        "\n",
        "tokens['Lemmatized Tokens'] = tokens['Tokens'].apply(lambda x: [token for token in x if token not in stop_words])\n",
        "tokens['Lemmatized Tokens'] = tokens['Lemmatized Tokens'].apply(lemmatize_text)\n",
        "tokens['Unique Words']=[list(set(content)) for content in tokens['Lemmatized Tokens']]\n",
        "\n",
        "# Display the DataFrame with stems and lemmas\n",
        "print(tokens[['Topic Title', 'Stemmed Tokens', 'Lemmatized Tokens']])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[['Topic Title', 'Stemmed Tokens', 'Lemmatized Tokens']].to_csv('/content/tokens.csv')"
      ],
      "metadata": {
        "id": "uIWo_CBzy3kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BHLovTcMe0"
      },
      "source": [
        "#vocabulary.csv: topic, unique words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = copy.deepcopy(tokens[['Topic Title', 'Unique Words']])"
      ],
      "metadata": {
        "id": "1sa4D3Vj0JNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVkbWG0kcNsK"
      },
      "outputs": [],
      "source": [
        "vocabulary.to_csv('/content/vocabulary.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZ0VfCxchI_"
      },
      "source": [
        "# Part-of-Speech Tagging with spaCy\n",
        "\n",
        "we use spaCy, a natural language processing library, to perform Part-of-Speech (POS) tagging on the tokenized words. POS tagging involves assigning grammatical categories (such as nouns, verbs, adjectives, etc.) to each word in a text.\n",
        "\n",
        "## Downloading spaCy Model\n",
        "\n",
        "The code starts by checking if the spaCy model 'en_core_web_sm' is already downloaded. If not, it downloads the model using the command `!python -m spacy download en_core_web_sm`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pos.csv: topic, data with pos tags"
      ],
      "metadata": {
        "id": "pYLkuQvV2ciI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = copy.deepcopy(tokens[['Topic Title','Tokens']])"
      ],
      "metadata": {
        "id": "Bn78n1xB2_E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI8CwN6lcm4z"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(tokens):\n",
        "    text = ' '.join(tokens)  # Join tokens into a single string\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "# Apply POS tagging to the 'Tokens' column\n",
        "pos['POS Tags'] = pos['Tokens'].apply(pos_tagging)\n",
        "\n",
        "# Display the DataFrame with POS tags\n",
        "print(pos.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvi71mqZcsAw"
      },
      "outputs": [],
      "source": [
        "pos[['Topic Title','POS Tags']].to_csv('/content/pos.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRL9J_y3cs-u"
      },
      "source": [
        "##readabilty.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Imb2mLWcx-9"
      },
      "source": [
        "# Calculating Readability Metrics for Text Content\n",
        "\n",
        "readability metrics are calculated for a given set of text content using the `textstat` library. The `calculate_readability_metrics` function applies various readability formulas to assess the complexity and understandability of the text.\n",
        "\n",
        "## Readability Metrics\n",
        "\n",
        "The following readability metrics are computed for each piece of text content:\n",
        "\n",
        "- **Flesch Reading Ease:** A score indicating how easy or difficult the text is to understand. Higher scores correspond to easier readability.\n",
        "\n",
        "- **Flesch-Kincaid Grade Level:** The grade level required to understand the text. It represents the number of years of education needed to comprehend the content.\n",
        "\n",
        "- **SMOG Index:** A measure of text readability based on the number of polysyllabic words. It estimates the years of education a person needs to understand the text.\n",
        "\n",
        "- **Coleman-Liau Index:** An index indicating the understandability of the text. It assesses the text's complexity based on characters per word and words per sentence.\n",
        "\n",
        "- **Automated Readability Index:** A formula for assessing the understandability of a text. It provides a score that corresponds to the U.S. school grade level.\n",
        "\n",
        "- **Dale-Chall Readability Score:** A readability measure that considers a list of familiar words. It estimates the ease of comprehension for readers with different education levels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readability=copy.deepcopy(corpus[['Topic Title','Topic Content']])"
      ],
      "metadata": {
        "id": "4XMnwiRY66Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Id1GAcoc2Vp"
      },
      "outputs": [],
      "source": [
        "# Install the required library if not installed\n",
        "!pip install textstat\n",
        "\n",
        "import textstat\n",
        "\n",
        "# Function to calculate readability metrics\n",
        "def calculate_readability_metrics(text):\n",
        "    readability_metrics =  textstat.flesch_kincaid_grade(text)\n",
        "    if readability_metrics <=8:\n",
        "      return readability_metrics\n",
        "    else: return readability_metrics\n",
        "\n",
        "# Apply the function to the 'Cleaned Title' column\n",
        "readability['Readability Metrics'] = readability['Topic Content'].apply(calculate_readability_metrics)\n",
        "\n",
        "# Display the DataFrame with readability metrics\n",
        "readability.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readability.to_csv('/content/readability.csv')"
      ],
      "metadata": {
        "id": "_1lfbZW074I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the test assign the readability level of the text se readability as y, use logistic regression to classify whether the data is simple or easy"
      ],
      "metadata": {
        "id": "WbJR4eOqSOad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data= copy.deepcopy(readability[['Topic Content','Readability Metrics']])"
      ],
      "metadata": {
        "id": "Th2NMMCuR6G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.head()"
      ],
      "metadata": {
        "id": "EpPms2AiSaio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbcj5Pft-THH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n"
      ],
      "metadata": {
        "id": "oF6C_DPRSap4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Data['features']=Data['Topic Content'].apply(lambda x : model.encode(x))"
      ],
      "metadata": {
        "id": "9Xd-K_OPSasn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data.head()"
      ],
      "metadata": {
        "id": "I8BMNoZuSavV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X=np.array(model.encode(Data['Topic Content']))\n",
        "Data=Data[['features','Readability Metrics']]"
      ],
      "metadata": {
        "id": "7klZvD1dkbs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "CvdbuJ9clQlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "Rr3w_3jFbLSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_iris  # Example dataset\n",
        "import numpy as np\n",
        "\n",
        "# Load a sample dataset (you can replace this with your dataset)\n",
        "# X = np.array(Data['features'])  # features\n",
        "y = np.array(Data['Readability Metrics'])  # target variable\n",
        "\n",
        "# Convert the target variable into a binary classification problem\n",
        "# For example, you can create two classes based on a threshold\n",
        "threshold = 0.5  # Set a threshold based on your problem\n",
        "y_binary = (y > threshold).astype(int)\n",
        "\n",
        "# Split data into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rogYKT2fcMXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "YN8sYza6lwVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence; adjust as needed\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the scores\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1 Score: {f1:.2f}')"
      ],
      "metadata": {
        "id": "-bqaKBL6h6tV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UTYCgpECMEVf",
        "tIoE8xwaMJNe",
        "gjLrmddiMNMs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}